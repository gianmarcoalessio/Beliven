{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gianmarco Alessio - 2024/02/22\n",
    "\n",
    "# AI Developer Junior – Test Medium Level [2] \n",
    "\n",
    "Progetto  di  Elaborazione  del  Linguaggio  Naturale  (NLP)  con  Python  e  Scikit-\n",
    "Learn \n",
    "\n",
    "**Descrizione  del  progetto**:  Hai  a  disposizione  un  dataset  contenente  recensioni  di \n",
    "film e le relative etichette di sentiment, ovvero se una recensione è positiva o negativa. \n",
    "Il tuo obiettivo è sviluppare un modello di machine learning per la classificazione dei \n",
    "sentiment  delle  recensioni  utilizzando  tecniche  di  NLP.  Deve  essere  in  grado  di \n",
    "raggiungere un'accuratezza di almeno l’85% nella classificazione dei sentiment. \n",
    "Compiti per il candidato: \n",
    "\n",
    "- Esplorazione dei dati: iniziare con un'analisi esplorativa dei dati per \n",
    "comprendere  la  distribuzione  delle  etichette  di  sentiment,  la  lunghezza  delle \n",
    "recensioni, ecc. \n",
    "- Preelaborazione  dei  dati:  eseguire  la  preelaborazione  dei  dati,  inclusa  la \n",
    "rimozione di stopwords, la tokenizzazione delle frasi, la creazione di vettori di \n",
    "parole (word embeddings) e la suddivisione del dataset in set di addestramento \n",
    "e di test. \n",
    "- Sviluppo del modello: progettare e allenare un modello di machine learning \n",
    "per la classificazione dei sentiment utilizzando Scikit-Learn. Puoi sperimentare \n",
    "con algoritmi come Support Vector Machines (SVM), Naive Bayes, o modelli di \n",
    "deep  learning  come  reti  neurali  ricorrenti  (RNN)  o  Long  Short-Term  Memory \n",
    "(LSTM) se si sentono confortevoli. \n",
    "- Valutazione del modello: valutare le prestazioni del modello utilizzando il set \n",
    "di test e misurare l'accuratezza, la precisione, il richiamo e l'F1-score. \n",
    "- Ottimizzazione  del  modello:  Se  il  modello  iniziale  non  raggiunge  l'85%  di \n",
    "accuratezza, cercare di ottimizzarlo. Puoi eseguire l'ottimizzazione dei \n",
    "parametri, provare diverse configurazioni di algoritmi o esplorare l'uso di word \n",
    "embeddings pre-addestrati. \n",
    "- Presentazione  dei  risultati:  presentare  i  risultati  del  progetto  e  spiegare  il \n",
    "processo seguito per sviluppare il modello. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importazione librerie utilizzate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wq/6qt2cn694p9fgrldr8r274tm0000gn/T/ipykernel_18264/4187975235.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caricamento del dataset chiamato `aclImdb` contenente recensioni di film e le relative etichette di sentiment, ovvero se una recensione è positiva o negativa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reviews(directory):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for label_type in ['pos', 'neg']:\n",
    "        dir_name = os.path.join(directory, label_type)\n",
    "        for file in os.listdir(dir_name):\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(dir_name, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    reviews.append(f.read())\n",
    "                    # 'pos' corrisponda a recensioni positive (etichetta = 1)\n",
    "                    # e 'neg' a recensioni negative (etichetta = 0)\n",
    "                    labels.append(1 if label_type == 'pos' else 0)\n",
    "    return reviews, labels\n",
    "\n",
    "\n",
    "train_reviews, train_labels = load_reviews('aclImdb/train/')\n",
    "test_reviews, test_labels = load_reviews('aclImdb/test/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Esplorazione dei dati\n",
    "\n",
    "- Distribuzione delle etichette di sentiment (50% positiva e 50% negativa)\n",
    "- Frequenza delle lunghezze delle recensioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converti in DataFrame\n",
    "train_data = pd.DataFrame({\n",
    "    'review': train_reviews,\n",
    "    'label': train_labels\n",
    "})\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'review': test_reviews,\n",
    "    'label': test_labels\n",
    "})\n",
    "\n",
    "# Funzioni per analizzare i dati\n",
    "def analyze_data(data, set_name=\"Train\"):\n",
    "    print(f\"Analisi del set {set_name}\")\n",
    "    print(data['label'].value_counts(normalize=True))\n",
    "    data['length'] = data['review'].apply(len)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data['length'], bins=50, alpha=0.7)\n",
    "    plt.title(f'Distribuzione della Lunghezza delle Recensioni - {set_name}')\n",
    "    plt.xlabel('Lunghezza della Recensione')\n",
    "    plt.ylabel('Frequenza')\n",
    "    plt.show()\n",
    "\n",
    "# Since we already have the word frequencies, we can directly generate the word clouds\n",
    "\n",
    "def create_word_cloud(frequencies, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(frequencies)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Funzione per plottare l'istogramma delle parole più frequenti\n",
    "def plot_word_frequencies(word_freq, title,top_n=100):\n",
    "    words = list(word_freq.keys())\n",
    "    frequencies = list(word_freq.values())\n",
    "    \n",
    "    # Ordinamento basato sulle frequenze\n",
    "    indices_sorted = np.argsort(frequencies)[::-1]\n",
    "    words_sorted = np.array(words)[indices_sorted][:top_n]\n",
    "    frequencies_sorted = np.array(frequencies)[indices_sorted][:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.bar(words_sorted, frequencies_sorted)\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequencies')\n",
    "    plt.show()\n",
    "\n",
    "# Function to load the labeled Bow features from the .feat file\n",
    "def load_labeled_bow(file_path):\n",
    "    data = []\n",
    "    indices = []\n",
    "    indptr = [0]\n",
    "    labels = []\n",
    "\n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            elements = line.split()\n",
    "            labels.append(int(elements[0]))\n",
    "            for item in elements[1:]:\n",
    "                index, value = item.split(':')\n",
    "                indices.append(int(index))\n",
    "                data.append(int(value))\n",
    "            indptr.append(len(indices))\n",
    "\n",
    "    # Create the CSR matrix\n",
    "    labeled_bow_matrix = csr_matrix((data, indices, indptr), dtype=int)\n",
    "    return labels, labeled_bow_matrix\n",
    "\n",
    "# Function to calculate word frequencies based on the BoW matrix and labels\n",
    "def calculate_word_frequencies(bow_matrix, labels, vocab):\n",
    "    # Split the matrix into positive and negative reviews based on labels\n",
    "    positive_matrix = bow_matrix[labels == 1]\n",
    "    negative_matrix = bow_matrix[labels == -1]\n",
    "    \n",
    "    # Sum up the word counts. csr_matrix automatically sums along axis 0 when using np.sum\n",
    "    positive_word_counts = np.sum(positive_matrix, axis=0)\n",
    "    negative_word_counts = np.sum(negative_matrix, axis=0)\n",
    "    \n",
    "    # Convert the matrix row to a flat array\n",
    "    positive_word_counts = np.array(positive_word_counts).flatten()\n",
    "    negative_word_counts = np.array(negative_word_counts).flatten()\n",
    "    \n",
    "    # Map indices to words using the provided vocabulary\n",
    "    positive_word_freq = {vocab[i]: count for i, count in enumerate(positive_word_counts) if count > 0}\n",
    "    negative_word_freq = {vocab[i]: count for i, count in enumerate(negative_word_counts) if count > 0}\n",
    "    \n",
    "    # Sort the words based on frequency\n",
    "    positive_word_freq = dict(sorted(positive_word_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "    negative_word_freq = dict(sorted(negative_word_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    return positive_word_freq, negative_word_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esegui l'analisi per i dati di addestramento e di test\n",
    "analyze_data(train_data, \"Addestramento\")\n",
    "analyze_data(test_data, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rimozione delle stopwords e tokenizzazione delle frasi per la rappresentazione della cloud word. Facendo uso delle librerie `nltk` e `wordcloud`, e `imdb.vocab` e `labeledBow.feat` per la rimozione delle stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = 'aclImdb/imdb.vocab'\n",
    "labeled_bow_path = 'aclImdb/train/labeledBow.feat'\n",
    "\n",
    "with open(vocab_path, 'r', encoding='utf-8') as file:\n",
    "    vocab = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Load the data\n",
    "labels, bow_matrix = load_labeled_bow(labeled_bow_path)\n",
    "\n",
    "# Check the shape of the matrix and the distribution of labels\n",
    "bow_matrix_shape = bow_matrix.shape\n",
    "labels_array = np.array(labels)\n",
    "adjusted_labels = np.where(labels_array < 5, -1, 1)\n",
    "\n",
    "# Count the occurrences of each sentiment\n",
    "positive_reviews_count = (adjusted_labels == 1).sum()\n",
    "negative_reviews_count = (adjusted_labels == -1).sum()\n",
    "\n",
    "(bow_matrix_shape, positive_reviews_count, negative_reviews_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of top words to consider\n",
    "top_n = 10 # create a plot for the different accuracies values in relationships of the words we consider \n",
    "positive_word_freq, negative_word_freq = calculate_word_frequencies(bow_matrix, adjusted_labels, vocab)\n",
    "\n",
    "# Download stopwords from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter out stopwords from the frequencies for more meaningful word clouds\n",
    "positive_word_freq_filtered = {word: freq for word, freq in positive_word_freq.items() if word not in stop_words and freq > 10}\n",
    "negative_word_freq_filtered = {word: freq for word, freq in negative_word_freq.items() if word not in stop_words and freq > 10}\n",
    "\n",
    "# Find common words between the top N words of both positive and negative reviews\n",
    "common_words = set(list(positive_word_freq_filtered.keys())[:top_n]).intersection(set(list(negative_word_freq_filtered.keys())[:top_n]))\n",
    "\n",
    "# Remove common words from the filtered frequencies\n",
    "positive_word_freq_filtered = {word: freq for word, freq in positive_word_freq_filtered.items() if word not in common_words}\n",
    "negative_word_freq_filtered = {word: freq for word, freq in negative_word_freq_filtered.items() if word not in common_words}\n",
    "\n",
    "# Generate and display the word clouds for the top words in positive and negative reviews\n",
    "create_word_cloud(positive_word_freq_filtered, 'Word Cloud for Positive Reviews')\n",
    "create_word_cloud(negative_word_freq_filtered, 'Word Cloud for Negative Reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Preelaborazione dei dati\n",
    "\n",
    "- Rimozione delle stopwords\n",
    "- Tokenizzazione delle frasi\n",
    "- Creazione di vettori di parole (word embeddings)\n",
    "- Suddivisione del dataset in set di addestramento e di test (già effettuata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumendo che `reviews` sia la tua lista di recensioni e `sentiments` le etichette di sentiment corrispondenti\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sviluppo del modello\n",
    "\n",
    "- Progettazione e allenamento di un modello di machine learning per la classificazione dei sentiment utilizzando Scikit-Learn.\n",
    "- Sperimentazione con algoritmi come Support Vector Machines (SVM), Naive Bayes, o modelli di deep learning come reti neurali ricorrenti (RNN) o Long Short-Term Memory (LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_vectorizer.fit_transform(train_reviews)\n",
    "X_test = tfidf_vectorizer.transform(test_reviews)\n",
    "\n",
    "y = train_labels\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allenamento con Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84     12500\n",
      "           1       0.86      0.80      0.83     12500\n",
      "\n",
      "    accuracy                           0.83     25000\n",
      "   macro avg       0.84      0.83      0.83     25000\n",
      "weighted avg       0.84      0.83      0.83     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione di Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alleanmento con Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = SVC(kernel='linear')\n",
    "model_svm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione di Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = model_svm.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversione dei dati in tensori PyTorch per la rete neurale riccorente RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversione dei dati in tensori PyTorch\n",
    "\n",
    "X_train_tensor = torch.tensor(X.toarray(), dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Creazione di un DataLoader per l'addestramento\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definizione del modello RNN con 2 layer LSTM e 1 layer fully connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        # Prendiamo l'output dell'ultimo time step\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definizione dei parametri di addestramento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione dei parametri\n",
    "vocab_size = 10000  # Dimensione del vocabolario\n",
    "output_size = 1  # Output binario (positivo/negativo)\n",
    "embedding_dim = 400  # Dimensione degli embeddings\n",
    "hidden_dim = 256  # Dimensione dello stato nascosto\n",
    "n_layers = 2  # Numero di layers RNN\n",
    "\n",
    "model = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "# Funzione di perdita e ottimizzatore\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Ciclo di addestramento\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allenamento del modello RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 5\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39msqueeze(), labels)\n\u001b[1;32m      8\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/beliven/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/beliven/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mSentimentRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     11\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m---> 12\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Prendiamo l'output dell'ultimo time step\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/beliven/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/beliven/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/beliven/lib/python3.11/site-packages/torch/nn/modules/rnn.py:553\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN_TANH\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 553\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_tanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    557\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_relu(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    558\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m    559\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output.squeeze(), labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione di RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione\n",
    "model.eval()  # Imposta il modello in modalità valutazione\n",
    "accuracy = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        output = model(inputs)\n",
    "        predictions = torch.round(output.squeeze())  # Arrotonda l'output a 0 o 1\n",
    "        correct = (predictions == labels).float()  # Confronta con le etichette vere\n",
    "        accuracy.append(correct.mean().item())\n",
    "\n",
    "print(f\"Accuracy: {np.mean(accuracy)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beliven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
